{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# NBA Prediction: Comprehensive Model Comparison\n",
                "\n",
                "This notebook compares traditional baselines, Machine Learning models, and Deep Learning models.\n",
                "\n",
                "## Models Evaluated\n",
                "### Baselines & ML\n",
                "1.  **Linear Regression**: Simple linear relationship.\n",
                "2.  **Rolling Average (Last 5)**: Moving average of recent performance.\n",
                "3.  **Random Forest**: Bagging ensemble.\n",
                "4.  **XGBoost**: Gradient Boosting.\n",
                "\n",
                "### Deep Learning\n",
                "5.  **SeqModel (Transformer)**: Time-series transformer using numerical stats sequences.\n",
                "6.  **GraphModel (CNN)**: Spatial-temporal CNN using shot chart heatmaps.\n",
                "7.  **MultiModal (CRNN)**: Hybrid model combining Shot Charts (CNN) and Stats (Transformer).\n",
                "\n",
                "## Metrics\n",
                "-   **RMSE**: Root Mean Squared Error (Lower is Better)\n",
                "-   **MAE**: Mean Absolute Error (Lower is Better)\n",
                "-   **R2 Score**: Coefficient of Determination (Higher is Better, max 1.0)\n",
                "-   **Std Dev**: Standard Deviation of true values (Target Variability)\n",
                "-   **NRMSE (norm)**: RMSE / Std Dev. If < 1.0, model is better than guessing the mean.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import os\n",
                "import json\n",
                "import torch\n",
                "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "from sklearn.linear_model import LinearRegression\n",
                "from xgboost import XGBRegressor\n",
                "from tqdm.notebook import tqdm\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "# Import Local Deep Learning Modules\n",
                "import sys\n",
                "sys.path.append('.')\n",
                "try:\n",
                "    from seqModel import NbaTransformer, createSequences\n",
                "    from graphModel import NbaCnn, createCnnSequences, loadAndPreprocessData\n",
                "    from multiModel import NbaMultimodal, createMultimodalSequences, loadAndPreprocessData as loadMultimodalData\n",
                "    DL_AVAILABLE = True\n",
                "except ImportError as e:\n",
                "    print(f\"Deep Learning modules not found: {e}\")\n",
                "    DL_AVAILABLE = False\n",
                "\n",
                "# Configuration\n",
                "GAMES_PATH = 'dataset/games.csv'\n",
                "SHOTS_PATH = 'dataset/shots.csv'\n",
                "TARGET_COLS = ['PTS', 'AST', 'REB']\n",
                "TEST_SEASON_ID = 22024\n",
                "\n",
                "# Set Style\n",
                "sns.set_theme(style=\"whitegrid\")\n",
                "plt.rcParams['figure.figsize'] = (14, 7)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading & Feature Engineering (ML Baselines)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Loading Data for ML Baselines...\")\n",
                "df = pd.read_csv(GAMES_PATH, low_memory=False)\n",
                "df['GAME_DATE'] = pd.to_datetime(df['GAME_DATE'], format='mixed')\n",
                "df = df.sort_values(['Player_ID', 'GAME_DATE']).reset_index(drop=True)\n",
                "for col in TARGET_COLS:\n",
                "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
                "df = df.dropna(subset=TARGET_COLS)\n",
                "\n",
                "def create_features(df):\n",
                "    df_eng = df.copy()\n",
                "    for target in TARGET_COLS:\n",
                "        # Baseline: Rolling Average (Last 5) - STRICTLY WITHIN SEASON\n",
                "        # We group by SEASON_ID to prevent cross-season leakage\n",
                "        df_eng[f'{target}_Roll5'] = df_eng.groupby(['Player_ID', 'SEASON_ID'])[target].shift(1).rolling(5).mean()\n",
                "        \n",
                "        # ML Features: Lags - STRICTLY WITHIN SEASON\n",
                "        for lag in [1, 2, 3]:\n",
                "            df_eng[f'{target}_Lag{lag}'] = df_eng.groupby(['Player_ID', 'SEASON_ID'])[target].shift(lag)\n",
                "            \n",
                "    return df_eng\n",
                "\n",
                "print(\"Generating Features (Strict Season)...\")\n",
                "df_features = create_features(df)\n",
                "df_clean = df_features.dropna().reset_index(drop=True)\n",
                "\n",
                "train_df = df_clean[df_clean['SEASON_ID'] != TEST_SEASON_ID].copy()\n",
                "test_df = df_clean[df_clean['SEASON_ID'] == TEST_SEASON_ID].copy()\n",
                "\n",
                "print(f\"ML Train: {len(train_df)}, ML Test: {len(test_df)}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Train & Evaluate ML Baselines"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results = []\n",
                "\n",
                "# Prepare features\n",
                "feature_cols = []\n",
                "for t in TARGET_COLS:\n",
                "    feature_cols += [f'{t}_Lag1', f'{t}_Lag2', f'{t}_Lag3', f'{t}_Roll5']\n",
                "\n",
                "models = {\n",
                "    'Linear Regression': LinearRegression(),\n",
                "    'RandomForest': RandomForestRegressor(n_estimators=50, max_depth=8, n_jobs=-1, random_state=42),\n",
                "    'XGBoost': XGBRegressor(n_estimators=100, learning_rate=0.1, n_jobs=-1, random_state=42)\n",
                "}\n",
                "\n",
                "for target in TARGET_COLS:\n",
                "    print(f\"Evaluating {target}...\")\n",
                "    y_test = test_df[target].values\n",
                "    std_dev = np.std(y_test)\n",
                "    \n",
                "    # 1. Rolling Baseline\n",
                "    pred_roll = test_df[f'{target}_Roll5'].values\n",
                "    rmse_roll = np.sqrt(mean_squared_error(y_test, pred_roll))\n",
                "    results.append({\n",
                "        'Target': target, 'Model': 'Rolling Avg (5)',\n",
                "        'RMSE': rmse_roll,\n",
                "        'MAE': mean_absolute_error(y_test, pred_roll),\n",
                "        'R2': r2_score(y_test, pred_roll),\n",
                "        'StdDev': std_dev,\n",
                "        'NRMSE': rmse_roll / std_dev\n",
                "    })\n",
                "    \n",
                "    # 2. ML Models\n",
                "    X_train = train_df[feature_cols]\n",
                "    y_train = train_df[target]\n",
                "    X_test = test_df[feature_cols]\n",
                "    \n",
                "    for name, model in models.items():\n",
                "        model.fit(X_train, y_train)\n",
                "        pred = model.predict(X_test)\n",
                "        rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
                "        results.append({\n",
                "            'Target': target, 'Model': name,\n",
                "            'RMSE': rmse,\n",
                "            'MAE': mean_absolute_error(y_test, pred),\n",
                "            'R2': r2_score(y_test, pred),\n",
                "            'StdDev': std_dev,\n",
                "            'NRMSE': rmse / std_dev\n",
                "        })\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Evaluate Deep Learning Models\n",
                "We load the latest checkpoints from `savedSeqModels`, `savedCnnModels`, and `savedMultimodalModels`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_latest_model_path(base_dir):\n",
                "    if not os.path.exists(base_dir): return None\n",
                "    subdirs = [os.path.join(base_dir, d) for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
                "    if not subdirs: return None\n",
                "    return max(subdirs, key=os.path.getmtime)\n",
                "\n",
                "def predict_batch(model, device, *inputs):\n",
                "    \"\"\"\n",
                "    Helper to run inference in batches to avoid OOM\n",
                "    \"\"\"\n",
                "    batch_size = 32\n",
                "    n_samples = len(inputs[0])\n",
                "    preds_list = []\n",
                "    \n",
                "    # inputs is a tuple of arrays, e.g. (xImg, xStat) or (xSeq,)\n",
                "    \n",
                "    for i in range(0, n_samples, batch_size):\n",
                "        # Slice batch\n",
                "        batch_inputs = [x[i:i+batch_size] for x in inputs]\n",
                "        # To Torch & Device\n",
                "        batch_tensors = [torch.FloatTensor(b).to(device) for b in batch_inputs]\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            batch_pred = model(*batch_tensors)\n",
                "            preds_list.append(batch_pred.cpu().numpy())\n",
                "            \n",
                "    return np.concatenate(preds_list, axis=0)\n",
                "\n",
                "def evaluate_dl_model(model_type, save_dir):\n",
                "    best_path = get_latest_model_path(save_dir)\n",
                "    if not best_path:\n",
                "        print(f\"No saved models found in {save_dir}\")\n",
                "        return\n",
                "    \n",
                "    print(f\"[{model_type}] Finding best model... Found: {best_path}\")\n",
                "    \n",
                "    try:\n",
                "        with open(os.path.join(best_path, 'config.json'), 'r') as f:\n",
                "            config = json.load(f)\n",
                "        \n",
                "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "        model = None\n",
                "        preds = None\n",
                "        y_truth = None\n",
                "        \n",
                "        # Step 1: Load & Preprocess Data\n",
                "        if model_type == 'SeqModel':\n",
                "            print(f\"[{model_type}] Step 1: Loading Data...\")\n",
                "            gamesData = pd.read_csv(GAMES_PATH, low_memory=False)\n",
                "            gamesData['GAME_DATE'] = pd.to_datetime(gamesData['GAME_DATE'], format='mixed')\n",
                "            for col in TARGET_COLS: gamesData[col] = pd.to_numeric(gamesData[col], errors='coerce')\n",
                "            gamesData = gamesData.dropna(subset=TARGET_COLS).sort_values(['Player_ID', 'GAME_DATE'])\n",
                "            \n",
                "            test_season_data = gamesData[gamesData['SEASON_ID'].astype(str) == str(TEST_SEASON_ID)]\n",
                "            train_season_data = gamesData[gamesData['SEASON_ID'].astype(str) != str(TEST_SEASON_ID)]\n",
                "            \n",
                "            # FULL Feature List (Must match training!)\n",
                "            featureCols = ['PTS', 'AST', 'REB', 'FGM', 'FGA', 'FG_PCT', 'FG3M', 'FG3A', 'FG3_PCT', \n",
                "                           'FTM', 'FTA', 'FT_PCT', 'OREB', 'DREB', 'STL', 'BLK', 'TOV', 'PF', \n",
                "                           'PLUS_MINUS', 'MIN', 'USG_PCT', 'OFF_RATING', 'DEF_RATING', 'PACE', 'TS_PCT']\n",
                "            \n",
                "            # Check if we need to extend featureCols manually if not using loadAndPreprocessData\n",
                "            # SeqModel notebook implementation above used hardcoded features, so we stick to it unless updated.\n",
                "            # But the Error was in MultiModal, so let's focus there.\n",
                "            \n",
                "            print(f\"[{model_type}] Step 2: Generating Sequences & Scaling...\")\n",
                "            xTrain, yTrain = createSequences(train_season_data, config['seqLength'], featureCols, TARGET_COLS)\n",
                "            xTest, yTest = createSequences(test_season_data, config['seqLength'], featureCols, TARGET_COLS)\n",
                "            \n",
                "            # Scale Features\n",
                "            scalerX = StandardScaler()\n",
                "            N_train, S, F = xTrain.shape\n",
                "            xTrainReshaped = xTrain.reshape(-1, F)\n",
                "            scalerX.fit(xTrainReshaped)\n",
                "            \n",
                "            N_test, S_test, F_test = xTest.shape\n",
                "            xTestScaled = scalerX.transform(xTest.reshape(-1, F)).reshape(N_test, S_test, F_test)\n",
                "            \n",
                "            # Scale Targets\n",
                "            scalerY = StandardScaler()\n",
                "            scalerY.fit(yTrain)\n",
                "            \n",
                "            print(f\"[{model_type}] Step 3: Loading Model...\")\n",
                "            model = NbaTransformer(\n",
                "                inputDim=len(featureCols),\n",
                "                dModel=config.get('dModel', 64),\n",
                "                nHead=config.get('nHead', 4),\n",
                "                numLayers=config.get('numLayers', 2),\n",
                "                outputDim=len(TARGET_COLS),\n",
                "                dropout=config.get('dropout', 0.1)\n",
                "            )\n",
                "            model.load_state_dict(torch.load(os.path.join(best_path, 'model.ckpt'), map_location=device))\n",
                "            model.to(device).eval()\n",
                "            \n",
                "            print(f\"[{model_type}] Step 4: Inference (Batched)...\")\n",
                "            preds = predict_batch(model, device, xTestScaled)\n",
                "            preds = scalerY.inverse_transform(preds)\n",
                "            y_truth = yTest\n",
                "\n",
                "        elif model_type == 'GraphModel':\n",
                "            print(f\"[{model_type}] Step 1: Loading Data...\")\n",
                "            gamesData, shotsGrouped, targetCols = loadAndPreprocessData(GAMES_PATH, SHOTS_PATH)\n",
                "            testGames = gamesData[gamesData['SEASON_ID'].isin([TEST_SEASON_ID])].copy()\n",
                "            trainGames = gamesData[~gamesData['SEASON_ID'].isin([TEST_SEASON_ID])].copy()\n",
                "            \n",
                "            print(f\"[{model_type}] Step 2: Generating Heatmap Sequences...\")\n",
                "            xTest, yTest = createCnnSequences(testGames, shotsGrouped, config['seqLength'], TARGET_COLS)\n",
                "            _, yTrain = createCnnSequences(trainGames, shotsGrouped, config['seqLength'], TARGET_COLS)\n",
                "            \n",
                "            scalerY = StandardScaler()\n",
                "            scalerY.fit(yTrain)\n",
                "            \n",
                "            print(f\"[{model_type}] Step 3: Loading Model...\")\n",
                "            inputCh = config['seqLength'] * 2\n",
                "            model = NbaCnn(outputDim=len(TARGET_COLS), inputChannels=inputCh)\n",
                "            model.load_state_dict(torch.load(os.path.join(best_path, 'model.ckpt'), map_location=device))\n",
                "            model.to(device).eval()\n",
                "            \n",
                "            print(f\"[{model_type}] Step 4: Inference (Batched)...\")\n",
                "            preds = predict_batch(model, device, xTest)\n",
                "            preds = scalerY.inverse_transform(preds)\n",
                "            y_truth = yTest\n",
                "\n",
                "        elif model_type == 'MultiModal':\n",
                "            print(f\"[{model_type}] Step 1: Loading Data...\")\n",
                "            gamesData, shotsGrouped, fCols, tCols = loadMultimodalData(GAMES_PATH, SHOTS_PATH, config['seqLength'])\n",
                "            testGames = gamesData[gamesData['SEASON_ID'].isin([TEST_SEASON_ID])].copy()\n",
                "            trainGames = gamesData[~gamesData['SEASON_ID'].isin([TEST_SEASON_ID])].copy()\n",
                "            \n",
                "            print(f\"[{model_type}] Step 2: Generating Sequences & Scaling...\")\n",
                "            xImgTest, xStatTest, yTest = createMultimodalSequences(testGames, shotsGrouped, config['seqLength'], fCols, tCols)\n",
                "            _, xStatTrain, yTrain = createMultimodalSequences(trainGames, shotsGrouped, config['seqLength'], fCols, tCols)\n",
                "\n",
                "            scalerX = StandardScaler()\n",
                "            N, S, F = xStatTrain.shape\n",
                "            scalerX.fit(xStatTrain.reshape(-1, F))\n",
                "            \n",
                "            N_t, S_t, F_t = xStatTest.shape\n",
                "            xStatTest = scalerX.transform(xStatTest.reshape(-1, F)).reshape(N_t, S_t, F_t)\n",
                "\n",
                "            scalerY = StandardScaler()\n",
                "            scalerY.fit(yTrain)\n",
                "\n",
                "            print(f\"[{model_type}] Step 3: Loading Model...\")\n",
                "            \n",
                "            # --- FIX: Handle Feature Mismatch for Old Models ---\n",
                "            # Try loading state dict to check weight shape\n",
                "            checkpoint = torch.load(os.path.join(best_path, 'model.ckpt'), map_location=device)\n",
                "            stat_weight_shape = checkpoint['statEncoder.0.weight'].shape # (32, input_dim)\n",
                "            ckpt_input_dim = stat_weight_shape[1]\n",
                "            \n",
                "            current_input_dim = len(fCols)\n",
                "            \n",
                "            if ckpt_input_dim != current_input_dim:\n",
                "                print(f\"\u26a0\ufe0f Warning: Model expects {ckpt_input_dim} features but data has {current_input_dim}.\")\n",
                "                print(f\"cutting features to match model...\")\n",
                "                # Slice features to match model\n",
                "                xStatTest = xStatTest[:, :, :ckpt_input_dim]\n",
                "                # Update input dimension for model init\n",
                "                input_dim_to_use = ckpt_input_dim\n",
                "            else:\n",
                "                input_dim_to_use = current_input_dim\n",
                "                \n",
                "            model = NbaMultimodal(\n",
                "                numStatFeatures=input_dim_to_use, \n",
                "                seqLength=config['seqLength'], outputDim=len(TARGET_COLS),\n",
                "                cnnEmbedDim=config['cnnEmbedDim'], statEmbedDim=config['statEmbedDim'],\n",
                "                dModel=config['dModel'], nHead=config['nHead'],\n",
                "                numLayers=config['numLayers'], dropout=config['dropout']\n",
                "            )\n",
                "            model.load_state_dict(checkpoint)\n",
                "            model.to(device).eval()\n",
                "\n",
                "            print(f\"[{model_type}] Step 4: Inference (Batched)...\")\n",
                "            preds = predict_batch(model, device, xImgTest, xStatTest)\n",
                "            preds = scalerY.inverse_transform(preds)\n",
                "            y_truth = yTest\n",
                "\n",
                "        # Calculate Metrics\n",
                "        if preds is not None:\n",
                "            for i, target in enumerate(TARGET_COLS):\n",
                "                y_true_col = y_truth[:, i]\n",
                "                y_pred_col = preds[:, i]\n",
                "                rmse = np.sqrt(mean_squared_error(y_true_col, y_pred_col))\n",
                "                mae = mean_absolute_error(y_true_col, y_pred_col)\n",
                "                r2 = r2_score(y_true_col, y_pred_col)\n",
                "                std_dev = np.std(y_true_col)\n",
                "                \n",
                "                results.append({\n",
                "                    'Target': target, 'Model': model_type,\n",
                "                    'RMSE': rmse, 'MAE': mae, 'R2': r2,\n",
                "                    'StdDev': std_dev, 'NRMSE': rmse/std_dev\n",
                "                })\n",
                "            print(f\"[{model_type}] Evaluation Complete.\")\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"Failed to evaluate {model_type}: {e}\")\n",
                "        import traceback\n",
                "        traceback.print_exc()\n",
                "\n",
                "if DL_AVAILABLE:\n",
                "    evaluate_dl_model('SeqModel', 'savedSeqModels')\n",
                "    evaluate_dl_model('GraphModel', 'savedCnnModels')\n",
                "    evaluate_dl_model('MultiModal', 'savedMultimodalModels')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results_df = pd.DataFrame(results)\n",
                "\n",
                "print(\"Leaderboard (Lowest RMSE by Target):\")\n",
                "for target in TARGET_COLS:\n",
                "    sub = results_df[results_df['Target'] == target].sort_values('RMSE')\n",
                "    display(sub.head(5))\n",
                "\n",
                "print(\"\\nDetailed Statistical Metrics:\")\n",
                "display(results_df[['Target', 'Model', 'RMSE', 'StdDev', 'NRMSE', 'R2']].sort_values(['Target', 'RMSE']))\n",
                "\n",
                "g = sns.catplot(\n",
                "    data=results_df, kind=\"bar\",\n",
                "    x=\"Target\", y=\"RMSE\", hue=\"Model\",\n",
                "    height=6, aspect=2, palette=\"magma\"\n",
                ")\n",
                "plt.title(\"Model Comparison: Baseline vs ML vs Deep Learning\")\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}