{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# NBA Prediction: Comprehensive Model Comparison\n",
                "\n",
                "This notebook compares traditional baselines, Machine Learning models, and Deep Learning models.\n",
                "\n",
                "## Models Evaluated\n",
                "### Baselines & ML\n",
                "1.  **Linear Regression**: Simple linear relationship.\n",
                "2.  **Rolling Average (Last 5)**: Moving average of recent performance.\n",
                "3.  **Random Forest**: Bagging ensemble.\n",
                "4.  **XGBoost**: Gradient Boosting.\n",
                "\n",
                "### Deep Learning\n",
                "5.  **SeqModel (Transformer)**: Time-series transformer using numerical stats sequences.\n",
                "6.  **GraphModel (CNN)**: Spatial-temporal CNN using shot chart heatmaps.\n",
                "\n",
                "## Metrics\n",
                "-   **RMSE**: Root Mean Squared Error (Primary Metric)\n",
                "-   **MAE**: Mean Absolute Error\n",
                "-   **Std Dev (Reference)**: If RMSE < Std Dev, model has predictive power.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import os\n",
                "import json\n",
                "import torch\n",
                "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "from sklearn.linear_model import LinearRegression\n",
                "from xgboost import XGBRegressor\n",
                "from tqdm.notebook import tqdm\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "# Import Local Deep Learning Modules\n",
                "import sys\n",
                "sys.path.append('.')\n",
                "# Note: These imports assume seqModel.py and graphModel.py are in the same folder\n",
                "try:\n",
                "    from seqModel import NbaTransformer, createSequences\n",
                "    from graphModel import NbaCnn, createCnnSequences, loadAndPreprocessData\n",
                "    DL_AVAILABLE = True\n",
                "except ImportError as e:\n",
                "    print(f\"Deep Learning modules not found: {e}\")\n",
                "    DL_AVAILABLE = False\n",
                "\n",
                "# Configuration\n",
                "GAMES_PATH = 'dataset/games.csv'\n",
                "SHOTS_PATH = 'dataset/shots.csv'\n",
                "TARGET_COLS = ['PTS', 'AST', 'REB']\n",
                "TEST_SEASON_ID = 22024\n",
                "\n",
                "# Set Style\n",
                "sns.set_theme(style=\"whitegrid\")\n",
                "plt.rcParams['figure.figsize'] = (14, 7)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading & Feature Engineering (ML Baselines)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_features(df):\n",
                "    df_eng = df.copy()\n",
                "    for target in TARGET_COLS:\n",
                "        # Baseline: Rolling Average (Last 5) - STRICTLY WITHIN SEASON\n",
                "        # We group by SEASON_ID as well to matching DL models behavior\n",
                "        df_eng[f'{target}_Roll5'] = df_eng.groupby(['Player_ID', 'SEASON_ID'])[target].shift(1).rolling(5).mean()\n",
                "        \n",
                "        # ML Features: Lags - STRICTLY WITHIN SEASON\n",
                "        # If we just shift, we might shift previous season data into current season game 1\n",
                "        # So we must group by Season\n",
                "        for lag in [1, 2, 3]:\n",
                "            df_eng[f'{target}_Lag{lag}'] = df_eng.groupby(['Player_ID', 'SEASON_ID'])[target].shift(lag)\n",
                "            \n",
                "    return df_eng\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Train & Evaluate ML Baselines"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results = []\n",
                "predictions = {} # Store test set predictions for visualization\n",
                "\n",
                "# Prepare features\n",
                "feature_cols = []\n",
                "for t in TARGET_COLS:\n",
                "    feature_cols += [f'{t}_Lag1', f'{t}_Lag2', f'{t}_Lag3', f'{t}_Roll5']\n",
                "\n",
                "models = {\n",
                "    'Linear Regression': LinearRegression(),\n",
                "    'RandomForest': RandomForestRegressor(n_estimators=50, max_depth=8, n_jobs=-1, random_state=42),\n",
                "    'XGBoost': XGBRegressor(n_estimators=100, learning_rate=0.1, n_jobs=-1, random_state=42)\n",
                "}\n",
                "\n",
                "for target in TARGET_COLS:\n",
                "    print(f\"Evaluating {target}...\")\n",
                "    y_test = test_df[target].values\n",
                "    std_dev = np.std(y_test)\n",
                "    \n",
                "    # 1. Rolling Baseline\n",
                "    pred_roll = test_df[f'{target}_Roll5'].values\n",
                "    results.append({\n",
                "        'Target': target, 'Model': 'Rolling Avg (5)',\n",
                "        'RMSE': np.sqrt(mean_squared_error(y_test, pred_roll)),\n",
                "        'MAE': mean_absolute_error(y_test, pred_roll),\n",
                "        'StdDev': std_dev\n",
                "    })\n",
                "    \n",
                "    # 2. ML Models\n",
                "    X_train = train_df[feature_cols]\n",
                "    y_train = train_df[target]\n",
                "    X_test = test_df[feature_cols]\n",
                "    \n",
                "    for name, model in models.items():\n",
                "        model.fit(X_train, y_train)\n",
                "        pred = model.predict(X_test)\n",
                "        results.append({\n",
                "            'Target': target, 'Model': name,\n",
                "            'RMSE': np.sqrt(mean_squared_error(y_test, pred)),\n",
                "            'MAE': mean_absolute_error(y_test, pred),\n",
                "            'StdDev': std_dev\n",
                "        })\n",
                "        \n",
                "        # Save predictions for visualization (only need one 'best' ML, let's keep XGB)\n",
                "        if name == 'XGBoost':\n",
                "            predictions[f'{target}_XGB'] = pred"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Evaluate Deep Learning Models\n",
                "We load the latest checkpoints from `savedSeqModels` and `savedCnnModels`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- STRICT ALIGNMENT HELPER ---\n",
                "def get_valid_dl_indices(df, seq_length):\n",
                "    \"\"\"Replicates the createSequences logic to find which games are actually predicted.\"\"\"\n",
                "    valid_indices = []\n",
                "    # Must match: Group by Player, Season -> sort date -> sliding window\n",
                "    # The target of the sliding window is the game at index (i + seqLength)\n",
                "    \n",
                "    # Ensure Sort\n",
                "    df_sorted = df.sort_values(['Player_ID', 'GAME_DATE'])\n",
                "    \n",
                "    groups = df_sorted.groupby(['Player_ID', 'SEASON_ID'])\n",
                "    for _, group in groups:\n",
                "        if len(group) <= seq_length: continue\n",
                "        \n",
                "        # The first 'seq_length' games are inputs only, not targets.\n",
                "        # Targets start from index 'seq_length'\n",
                "        # e.g. seq=5. Indices 0,1,2,3,4 are input. Index 5 is first target.\n",
                "        # So we take group.iloc[seq_length:]\n",
                "        \n",
                "        valid_subset = group.iloc[seq_length:]\n",
                "        # We'll use a composite key for matching\n",
                "        keys = valid_subset['Player_ID'].astype(str) + '_' + valid_subset['Game_ID'].astype(str)\n",
                "        valid_indices.extend(keys.tolist())\n",
                "        \n",
                "    return set(valid_indices)\n",
                "\n",
                "# 1. Determine the 'Common Valid Set'\n",
                "# We assume seqLength=5 is the standard (or read from config if loaded)\n",
                "# For safety, we'll calculate it based on the ML Test DF which already dropped 5 rows/group via rolling.\n",
                "# Actually, 'df_clean' in ML part already did dropna(). \n",
                "# Rolling(5) results in first 4 NaNs. The 5th element is valid? \n",
                "# Rolling(5) at index T uses T, T-1, T-2, T-3, T-4. \n",
                "# We Shift(1). So at T, we use T-1...T-5. \n",
                "# So first 5 games (indices 0-4) will be NaN. Index 5 is valid.\n",
                "# This matches DL logic (targets start at index 5).\n",
                "\n",
                "# So, 'test_df' (which is df_clean filtered by season) SHOULD be perfectly aligned already\n",
                "# IF strict season grouping was used in feature engineering (which we just patched).\n",
                "\n",
                "# Let's verify and force filter just in case DL model uses different seqLength\n",
                "print(\"Alignment Check...\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_latest_model_path(base_dir):\n",
                "    if not os.path.exists(base_dir): return None\n",
                "    subdirs = [os.path.join(base_dir, d) for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
                "    if not subdirs: return None\n",
                "    return max(subdirs, key=os.path.getmtime)\n",
                "\n",
                "def evaluate_dl_model(model_type, save_dir):\n",
                "    best_path = get_latest_model_path(save_dir)\n",
                "    if not best_path:\n",
                "        print(f\"No saved models found in {save_dir}\")\n",
                "        return\n",
                "    \n",
                "    print(f\"Time to evaluate {model_type}... Loading from {best_path}\")\n",
                "    \n",
                "    try:\n",
                "        with open(os.path.join(best_path, 'config.json'), 'r') as f:\n",
                "            config = json.load(f)\n",
                "            \n",
                "        # Load Data (Re-using DL preprocessing)\n",
                "        # NOTE: This creates data fresh, so it might be slightly different dim than ML DF if logic differs\n",
                "        # But it should be the same TEST_SEASON\n",
                "        \n",
                "        if model_type == 'SeqModel':\n",
                "            gamesData = pd.read_csv(GAMES_PATH, low_memory=False)\n",
                "            # Preprocess is embedded in createSequences for SeqModel usually or separate?\n",
                "            # Checking seqModel.py structure... it does cleaning inline usually. \n",
                "            # Let's simple filter\n",
                "            gamesData['GAME_DATE'] = pd.to_datetime(gamesData['GAME_DATE'])\n",
                "            for col in TARGET_COLS: gamesData[col] = pd.to_numeric(gamesData[col], errors='coerce')\n",
                "            gamesData = gamesData.dropna(subset=TARGET_COLS).sort_values(['Player_ID', 'GAME_DATE'])\n",
                "            \n",
                "            test_season_data = gamesData[gamesData['SEASON_ID'].astype(str) == str(TEST_SEASON_ID)]\n",
                "            \n",
                "            # Create Sequences\n",
                "            # Caution: We need training scaler to inverse transform!\n",
                "            # We assume we re-fit scaler on TRAIN part to simulate production usage\n",
                "            train_season_data = gamesData[gamesData['SEASON_ID'].astype(str) != str(TEST_SEASON_ID)]\n",
                "            \n",
                "            # Fit Scaler\n",
                "            scaler = StandardScaler()\n",
                "            scaler.fit(train_season_data[TARGET_COLS])\n",
                "            \n",
                "            featureCols = ['PTS', 'AST', 'REB', 'FG_PCT', 'FG3_PCT', 'FT_PCT', 'PLUS_MINUS']\n",
                "            # Generate Test Sequences\n",
                "            xTest, yTest = createSequences(test_season_data, config['seqLength'], featureCols, TARGET_COLS)\n",
                "            \n",
                "            # Model\n",
                "            model = NbaTransformer(\n",
                "                numFeatures=len(featureCols), seqLength=config['seqLength'], outputDim=len(TARGET_COLS),\n",
                "                d_model=config.get('d_model', 64), nhead=config.get('nHead', 4),\n",
                "                numLayers=config.get('numLayers', 2), dropout=config.get('dropout', 0.1)\n",
                "            )\n",
                "            \n",
                "        elif model_type == 'GraphModel':\n",
                "            # Load Data using helper\n",
                "            gamesData, shotsGrouped, targetCols = loadAndPreprocessData(GAMES_PATH, SHOTS_PATH)\n",
                "            testGames = gamesData[gamesData['SEASON_ID'].isin([TEST_SEASON_ID])].copy()\n",
                "            trainGames = gamesData[~gamesData['SEASON_ID'].isin([TEST_SEASON_ID])].copy()\n",
                "            \n",
                "            xTest, yTest = createCnnSequences(testGames, shotsGrouped, config['seqLength'], TARGET_COLS)\n",
                "            \n",
                "            # Scaler\n",
                "            yTrain = createCnnSequences(trainGames, shotsGrouped, config['seqLength'], TARGET_COLS)[1]\n",
                "            scaler = StandardScaler()\n",
                "            scaler.fit(yTrain)\n",
                "\n",
                "            # Model\n",
                "            # Plan A Check: Input Channels = seqLength * 2\n",
                "            inputCh = config['seqLength'] * 2\n",
                "            model = NbaCnn(outputDim=len(TARGET_COLS), inputChannels=inputCh)\n",
                "\n",
                "        # Load Weights\n",
                "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "        model.load_state_dict(torch.load(os.path.join(best_path, 'model.ckpt'), map_location=device))\n",
                "        model.to(device)\n",
                "        model.eval()\n",
                "        \n",
                "        # Inference\n",
                "        test_inputs = torch.FloatTensor(xTest).to(device)\n",
                "        with torch.no_grad():\n",
                "            preds_scaled = model(test_inputs).cpu().numpy()\n",
                "        \n",
                "        preds_original = scaler.inverse_transform(preds_scaled)\n",
                "        \n",
                "        # Metrics\n",
                "        for i, target in enumerate(TARGET_COLS):\n",
                "            y_true = yTest[:, i]\n",
                "            y_pred = preds_original[:, i]\n",
                "            \n",
                "            rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
                "            mae = mean_absolute_error(y_true, y_pred)\n",
                "            std_dev = np.std(y_true)\n",
                "            \n",
                "            results.append({\n",
                "                'Target': target, 'Model': model_type,\n",
                "                'RMSE': rmse, 'MAE': mae, 'StdDev': std_dev\n",
                "            })\n",
                "            \n",
                "    except Exception as e:\n",
                "        print(f\"Failed to evaluate {model_type}: {e}\")\n",
                "\n",
                "if DL_AVAILABLE:\n",
                "    evaluate_dl_model('SeqModel', 'savedSeqModels')\n",
                "    evaluate_dl_model('GraphModel', 'savedCnnModels')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Final Comparison Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results_df = pd.DataFrame(results)\n",
                "\n",
                "print(\"Leaderboard (Lowest RMSE by Target):\")\n",
                "for target in TARGET_COLS:\n",
                "    sub = results_df[results_df['Target'] == target].sort_values('RMSE')\n",
                "    display(sub.head(3))\n",
                "\n",
                "g = sns.catplot(\n",
                "    data=results_df, kind=\"bar\",\n",
                "    x=\"Target\", y=\"RMSE\", hue=\"Model\",\n",
                "    height=6, aspect=2, palette=\"magma\"\n",
                ")\n",
                "plt.title(\"Model Comparison: Baseline vs ML vs Deep Learning\")\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}